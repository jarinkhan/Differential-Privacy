Differential privacy (DP) is a way to preserve the privacy
of individuals in a dataset while preserving the overall
usefulness of such a dataset. Ideally, someone shouldnâ€™t be
able to tell the difference between one dataset and a parallel
one with a single point removed. It guarantees to provide the
privacy of data when analyzing and sharing sensitive data.
To do this, randomized algorithms are used to add noise
to the data. The idea of adding noise to the data before
analyzing it is that the output of the analysis does not reveal
any individual-level information
